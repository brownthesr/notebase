
\documentclass[12pt]{article}
% This package simply sets the margins to be 1 inch.
\usepackage[margin=1in]{geometry}
\usepackage{enumerate}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{marvosym}% This adds the contradiction command (\Lightning)
% These packages include nice commands from AMS-LaTeX

% Make the space between lines slightly more
% generous than normal single spacing, but compensate
% so that the spacing between rows of matrices still
% looks normal.  Note that 1.1=1/.9090909...
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\arraystretch}{.91}

% Define an environment for exercises.
\newenvironment{exercise}[1]{\vspace{.1in}\noindent\textbf{Exercise #1 \hspace{.05em}}}{}

% define shortcut commands for commonly used symbols
\newcommand{\R}{\mathbb{R}}
\newcommand{\dt}{\Delta t}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\nR}{\mathcal{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\tr}{\text{tr}}
\newcommand{\xk}{x_k}
\newcommand{\yk}{y_k}
\newtheorem*{remark}{Remark}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\vsspan}{span}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{flushright}
	\textsc{Drake Brown}  \\
	Date Apr 14, 2025
\end{flushright}
\begin{center}
	Homework 1
\end{center}
% Exercise 1.1
\begin{exercise}{1.1}
	a) Let $x\in \text{range}(AB)$ then $\exists y$ with $x=ABy$ thus we know that $x=A(By)$ set $z=By$ thus $x=Az$ so $x\in \text{range}(A)$

	b) Let $x\in \text{corange}(AB)$ then $\exists y$ with $x=(AB)^\star y$ thus we know that $x=B^\star(Ay)$ set $z=A^\star y$ thus $x=B^\star z$ so $x\in \text{corange}(B)$

	c) Let $x\in \text{kernel}(B)$ then $Bx=0$ thus $ABx=A0=0$ so $x\in \text{kernel} (AB)$

	d) Let $x\in \text{cokernel}(A)$ then $A^\star x=0$ thus $(AB)^\star x= B^\star A^\star x=B^\star0=0$ so $x\in \text{cokernel} (AB)$
\end{exercise}

% Exercise 1.2
\begin{exercise}{1.2}
	From the definition lets compute $RC$
	\begin{align}
		(RC)_{k,l} = \sum_{q=1}^{n}R_{k,q}C_{q,l}
	\end{align}
	From here note that $R_{k,:}$ is just the kth row of R and $C_{:,l}$ is merely the lth collumn of C. Thus we are just taking an inner product here between the kth row of R and the lth column of C

	Thus
	\begin{align}
		(RC)_{k,l} =r_k^Tc_l
	\end{align}

	b) to do this we follow the definition note that:
	\begin{align}
		(CR)_{k,l} = \sum_q C_{k,q}R_{q,l}
	\end{align}
	If we let l range over $1,L$ then writing this in vector form we get:
	\begin{align}
		(CR)_{k,:} = \sum_q C_{k,q}R_{q,:}
	\end{align}
	Where here $R_{q,:}$ is the vector $r_q^T$:
	\begin{align}
		(CR)_{k,:} = \sum_q C_{k,q}r_q^T
	\end{align}
	Now if we let k range from $1,K$
	:
	\begin{align}
		(CR)_{:,:} = \sum_q C_{:,q}r_q^T
	\end{align}
	now we know that $ C_{:,q}$ is just $c_q$ thus:
	\begin{align}
		(CR)_{:,:} = \sum_q c_{q}r_q^T
		(CR) = \sum_q c_{q}r_q^T
	\end{align}
	c) The maximal possible rank would thus have to be just $n$ because each of these matrix are rank one and the sum of $n$ rank one matrices can be at most rank $n$ (Each matrix contributes one rank).

	To prove this remember that the rank of a matrix is the dimension of its range. taking some arbitrary vector x we have

	\begin{align}
		CRx=\sum_{j}c_jr_j^Tx
		=\sum_{j}c_j(r_j^Tx)
		=\sum_{j}(r_j^Tx)c_j
	\end{align}
	This is just a linear combination of the $c_j$ since there are only $n$ $c_j$ This can be at most $n$ dimensional. Thus the range is at most $n$ dimensional and the rank is at most n.
\end{exercise}

% Exercise 1.3
\begin{exercise}{1.3}
	the necessary and sufficient conditions are that $A$ is invertible.

	sufficient proof Assume A is invertible then:

	\begin{align}
		\norm{x+y}_A=\norm{A(x+y)}=\norm{Ax+Ay}\leq \norm{Ax}+\norm{Ay}=\norm{x}_A+\norm{y}_A \\
		\norm{cx}_A=\norm{A(cx)}=\norm{cAx}=|c|\norm{Ax}=|c|\norm{x}_A
	\end{align}
	note that since A is invertible $Ax=0$ iff $x=0$ thus we know that if $x\neq 0,Ax=y\neq 0$:
	\begin{align}
		\norm{x}_A=\norm{Ax}=\norm{y}> 0
	\end{align}
	by definition of norm and if $x=0, Ax=0 $
	\begin{align}
		\norm{x}_A=\norm{Ax}=\norm{0}=0
	\end{align}
	So all of the condition for a norm are satisfied

	To prove the necessary condition assume that $\norm{-}_A$ is a norm. Assume BWOC that A is singular. Since A is singular $\exists x, Ax=0$ with $x\neq 0$

	Then taking this x $\norm{x}_A=\norm{Ax}=\norm{0}=0$

	But this is a contradiction since x is nonzero. So A must be Nonsingular
\end{exercise}
% Exercise 1.4
\begin{exercise}{1.4}
	First note that
	\begin{align}
		\norm{x}^2_2=\sum_i |x_i|^2\leq \sum_i \max_k|x_k|^2 \leq n\max_k |x_k^2|=n \norm{x}_\infty^2
		\norm{x}^2_2=n\norm{x}_\infty^2    \\
		\norm{x}_2=\sqrt{n}\norm{x}_\infty \\
	\end{align}
	Now note that:
	\begin{align}
		\norm{x}_2^2=\sum_i |x_i|^2\geq \max_i|x_i|^2=\norm{x}_\infty^2
	\end{align}
	So in our case $c=1$ and $k=\sqrt{n}$

	Choose $x=1$ The ones vector for the first inequality with k and notice $\norm{1}_2=\sqrt{n}$ furthermore $\norm{1}_\infty=1$ thus $\norm{x}_2=\sqrt{n}\norm{x}_\infty$

	For the c inequality choose $x=e_1$ Note that $\norm{e_1}_1=1$ and $\norm{e_1}_2=\sqrt{1+0+0+\dots}=1$, thus $\norm{x}_1=\norm{x}_2$
\end{exercise}
% Exercise 1.5
\begin{exercise}{1.5}
	\begin{align}
		\norm{Ax}_1=\sum_{i}|\sum_{j}A_{ij}x_j|\leq \sum_i\sum_j |A_{ij}||x_j|                        \\
		= \sum_j\sum_i |A_{ij}||x_j|=\sum_j |x_j|\sum_{i}|A_{ij}|\leq \sum_j|x_j|\max_k\sum_i|A_{ik}| \\
		=(\sum_j|x_j|)\max_k\sum_i|A_{ik}|                                                            \\
		=\norm{x}_1\max_k\sum_i|A_{ik}|                                                               \\
		\implies                                                                                      \\
		\frac{\norm{Ax}_1}{\norm{x}_1   }\leq \max_k\sum_i|A_{ik}|                                    \\
	\end{align}
	Note that since we need to take a supremum over all possible x values we can pick an x that achieves this bound namely $e_k$ where k corresponds to the largest collumn sum thus:
	\begin{align}
		\norm{A}_1=	\sup_x \frac{\norm{Ax}_1}{\norm{x}_1   }= \max_k\sum_i|A_{ik}|=\max_{j\in[n]}\norm{c_j}_1 \\
	\end{align}

	To prove the infinity norm statement note that:
	\begin{align}
		\norm{Ax}_\infty = \sup_i|\sum_{j}A_{ij}x_j| \\
		\leq \sup_i \sum_j|A_{ij}||x_j|              \\
		\leq \sup_i \sum_j|A_{ij}|\norm{x}_\infty
	\end{align}
	This means that:
	\begin{align}
		\frac{\norm{Ax}_\infty}{\norm{x}_\infty}\leq \sup_i \sum_j|A_{ij}|
	\end{align}
	We can actually choose an x that achieve this bound as well.

	Namely choose $x=\left[\text{sign}(A_{i1}),\text{sign}(A_{i2},\dots,\text{sign}(A_{in})\right]$

	Thus:
	\begin{align}
		\norm{A}_\infty =	\sup_x \frac{\norm{Ax}_\infty}{\norm{x}_\infty   }=\sup_i \sum_j|A_{ij}|=\max_{j\in[m]}\norm{r_j}_1
	\end{align}
\end{exercise}
% Exercise 1.6
\begin{exercise}{1.6}
	To prove this take the definition of the induced norm (Note let $A\in \R^{m\times k}, B\in \R^{k\times n}$)
	\begin{align}
		\norm{AB} = \max_x \frac{\norm{ABx}_m}{\norm{x}_n}                        \\
		= \max_x \frac{\norm{A(Bx)}_m}{\norm{x}_n}                                \\
		= \max_x \frac{\norm{A(Bx)}_m}{\norm{Bx}_k}\frac{\norm{Bx}_k}{\norm{x}_n} \\
	\end{align}
	Setting $y=Bx$
	\begin{align}
		= \max_x \frac{\norm{A(y)}_m}{\norm{y}_k}\frac{\norm{Bx}_k}{\norm{x}_n}          \\
		\leq \max_x\max_y \frac{\norm{A(y)}_m}{\norm{y}_k}\frac{\norm{Bx}_k}{\norm{x}_n} \\
		= \max_y\frac{\norm{A(y)}_m}{\norm{y}_k}\max_x\frac{\norm{Bx}_k}{\norm{x}_n}     \\
		= \norm{A}\norm{B}
	\end{align}

	For the frobenius norm note that:
	\begin{align}
		\norm{AB}_F^2=\sum_{i}\sum_j (AB)_{ij}                 \\
		=\sum_{i}\sum_j \sum_{k}|A_{ik}B_{kj}|^2               \\
		=\sum_{i}\sum_j  |A_{i,:}B_{:,j}|^2                    \\
		\leq \sum_i\sum_j \norm{A_{i,:}}^2\norm{B_{:,j}}^2     \\
		\leq (\sum_i \norm{A_{i,:}}^2)(\sum_j\norm{B_{:,j}}^2) \\ =(\sum_i \sum_k A_{i,k}^2)(\sum_i \sum_k B_{k,i}^2)    \\
		=\norm{A}_F^2\norm{B}^2_F
	\end{align}
	In this we used the cauchy-schwarts inequality. Taking square roots of both sides yields the theorem.
\end{exercise}


\end{document}
