\documentclass[12pt]{article}

% Margins
\usepackage[margin=1in]{geometry}

% AMS math packages
\usepackage{amsmath,amsthm,amssymb,amsfonts,mathtools}

% For contradiction symbol
\usepackage{marvosym}

% Line spacing
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\arraystretch}{.91}

% Common math shortcuts
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newenvironment{exercise}[1]{\vspace{.1in}\noindent\textbf{Exercise #1 \hspace{.05em}}}{}
\newcommand{\tr}{\text{tr}}
\newcommand{\calP}{\mathcal{P}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\vsspan}{span}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Floor/ceiling
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

% --- Theorem-style environments ---
\newtheorem{theorem}{Theorem}[section] % numbered within sections
\newtheorem{lemma}[theorem]{Lemma}     % same counter as theorems
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{flushright}
	\textsc{Drake Brown}  \\
	Date: Apr 14, 2025
\end{flushright}
\begin{center}
	Homework 1
\end{center}

% Exercise 3.1
\begin{exercise}{3.1}
	Note that by problem 4 we know that any matrix is orthonormally similar to an upper triangular matrix with the eigenvalues on the diagonal. So take $A$:
	\begin{align}
		A=UTU^*
	\end{align}
	Now note:
	\begin{align}
		\det(A)=\det(U)\det(T)\det(U^*) \\
		=\det(U)\det(U^*)\det(T)        \\
		=\det(UU^*)\det(T)=\det(I)\det(T)=\det(T)
	\end{align}
	now remember that the determinant of an upper triangular matrix is just the product of the diagonal entries. Since the diagonal entries in this case are the eigenvalues then:
	\begin{align}
		\det(A)=\det(T)=\prod_{i=1}^n\lambda_i
	\end{align}

	Similarly take the trace
	\begin{align}
		tr(A)=tr(UTU^*)=tr((UT)U^*)=tr(U^*(UT))=tr(U^*UT)=tr(T)
	\end{align}
	so once again the trace of $T$ is just the sum of the diagonal entries which just so happens to be the eigenvalues:
	\begin{align}
		\tr(A)=\tr(T)=\sum\limits_{k=1}^{n}(\lambda_k)
	\end{align}
\end{exercise}

% Exercise 3.2
\begin{exercise}{3.2}
	to do this take an arbitrary eigenpair $(\lambda,v)$ then note that:
	\begin{align}
		Av=\lambda v
	\end{align}
	in particular for the largest entry $v_i$ of v:
	\begin{align}
		\sum_{j=1}^nA_{ij}v_j=\lambda v_i             \\
		\sum_{j\neq i}A_{ij}v_j=\lambda v_i-A_{ii}v_i \\
		\sum_{j\neq i}A_{ij}v_j=(\lambda -A_{ii})v_i  \\
	\end{align}
	now note that we chose $i$ such that $v_i>v_j$ for any other J this means that $\frac{v_j}{v_i}<1$ thus:
	\begin{align}
		(\lambda -A_{ii})v_i = \sum_{j\neq i}A_{ij}v_j                  \\
		(\lambda -A_{ii}) = \sum_{j\neq i}A_{ij} \frac{v_j}{v_i}        \\
		|\lambda -A_{ii}| \leq \sum_{j\neq i}|A_{ij}| |\frac{v_j}{v_i}| \\
		| \leq \sum_{j\neq i}|A_{ij}|                                   \\
	\end{align}

	So the distance from this eigenvalue to the diagonal element is less than the given sum. thus $\lambda\in D_i$. Since lambda was arbitrary each lambda coulda be in a different $D_i$ but they are all in at least one by the preceding proof. Thus they are all in the union of $D_i$.



\end{exercise}

% Exercise 3.3
\begin{exercise}{3.3}
	Easy proof: Because we proved it in four we know every matrix is unitarily similar to an upper triangular matrix with the eigenvalues on the diagonal. From here note that if A is skew hermitian:
	\begin{align}
		A=-A^*           \\
		UTU^*=-(UTU^*)^* \\
		UTU^*=-UT^{*}U^* \\
		T=-T^{*}         \\
	\end{align}
	From this we know that first of all all of the diagonal elements equal their negative conjugate (meaning that they all must be imaginary). Secondly we know that $T$ is upper triangular but $-T^*$ is then lower triangular as a result both of them must be diagonal since they are equal. Thus T is diagonal and $A=UTU^*$

	Longer proof since they probably want us to use decompressing the matrix:

	First of all note that if $\lambda,v$ is an eigenpair of $A$ then:(choosing v to be unit length)
	\begin{align}
		\lambda = \lambda\langle v,v\rangle                                                        \\
		=\langle \lambda v,v\rangle                                                                \\
		=\langle Av,v\rangle                                                                       \\
		=\langle v,A^*v\rangle                                                                     \\
		=\langle v,-Av\rangle = \langle v,-\lambda v\rangle= \overline{-\lambda}\langle v,v\rangle \\
		=\overline{-\lambda}
	\end{align}

	So it any eigenvalue is equal to its conjugate negated, this means that it must be imaginary.

	Furthermore take any matrix A. We know that it has at least one eigenvalue since it has a characteristic polynomial. Take that eigenvalue $\lambda_1, v_1$ and corresponding unit eigenvector. construct $Q_1=[v_1,q_2,\dots,q_n]$ then note that:
	\begin{align}
		Q_1^*AQ_1=Q_1 % 1 x 4 BMatrix 
		\begin{bmatrix}
			Av_1 & Aq_2 & \dots & Aq_n
		\end{bmatrix}                                                                   \\
		=Q_1 % 1 x 4 BMatrix 
		\begin{bmatrix}
			\lambda_1 v_1 & Aq_2 & \dots & Aq_n
		\end{bmatrix}                                                          \\
		=
		\begin{bmatrix}
			\langle \lambda_1v_1,v_1\rangle & \langle Aq_2,v_1\rangle & \dots  & \langle Aq_n,v_1\rangle \\
			\langle\lambda_1v_1,q_2\rangle  & \langle Aq_2,q_2\rangle & \dots  & \vdots                  \\
			\vdots                          & \vdots                  & \ddots & \vdots                  \\
			\langle\lambda_1v_1,q_n\rangle  & \dots                   & \dots  & \langle Aq_n,q_n\rangle
		\end{bmatrix} \\
		\begin{bmatrix}
			\langle \lambda_1v_1,v_1\rangle & \langle Aq_2,v_1\rangle & \dots  & \langle Aq_n,v_1\rangle \\
			0                               & \langle Aq_2,q_2\rangle & \dots  & \vdots                  \\
			\vdots                          & \vdots                  & \ddots & \vdots                  \\
			0                               & \dots                   & \dots  & \langle Aq_n,q_n\rangle
		\end{bmatrix} \\
	\end{align}
	Note from there that this new matrix $Q_1^*AQ_1$ is also skew hermitian $-A^*=-(Q_1^*AQ_1)^*=-Q_1^*A^*Q_1=Q_1^*(-(-A))Q_1=Q_1^*AQ_1$. Thius this lower block $A_1$ is also skew hermitian. and the upper right part must be zero:
	\begin{align}
		\begin{bmatrix}
			\langle \lambda_1v_1,v_1\rangle & 0                       & \dots  & 0                       \\
			0                               & \langle Aq_2,q_2\rangle & \dots  & \vdots                  \\
			\vdots                          & \vdots                  & \ddots & \vdots                  \\
			0                               & \dots                   & \dots  & \langle Aq_n,q_n\rangle
		\end{bmatrix} \\
		=% 2 x 2 BMatrix 
		\begin{bmatrix}
			\lambda_1 & 0   \\
			0         & A_1
		\end{bmatrix}
	\end{align}

	Furthermore we can repeat this process again. Note that $A_1$ thus has at least one eigenpair $v_2,\lambda_2$ construct an orthonormal basis $U_2=[v_2,z_2,\dots,z_{n-1}]\in \R^{(n-1)\times (n-1)}$

	From here take $Q_2=% 2 x 2 BMatrix 
		\begin{bmatrix}
			1 & 0   \\
			0 & U_2
		\end{bmatrix}$
	Then notice that:
	\begin{align}
		Q_2^*(Q_1^*AQ_1)Q_2 \\
		= % 2 x 2 BMatrix 
		\begin{bmatrix}
			I & 0     \\
			0 & U_2^*
		\end{bmatrix} % 2 x 2 BMatrix 
		\begin{bmatrix}
			\lambda_1 & 0   \\
			0         & A_1
		\end{bmatrix} % 2 x 2 BMatrix 
		\begin{bmatrix}
			I & 0   \\
			0 & u_2
		\end{bmatrix}      \\
		= % 2 x 2 BMatrix 
		\begin{bmatrix}
			\lambda_1 & 0           \\
			0         & U_2^*A_1U_2
		\end{bmatrix}
	\end{align}
	then by the same argument as before but this time for $A_1$
	\begin{align}
		= % 3 x 3 BMatrix 
		\begin{bmatrix}
			\lambda_1 & 0         & 0   \\
			0         & \lambda_2 & 0   \\
			0         & 0         & A_2
		\end{bmatrix}
	\end{align}
	For some new matrix $A_2$ (this is because we took the eigenvector of $A_1$). We can keep continuing this process until completion. We then have:
	\begin{align}
		(\prod_i Q_i)^*A\prod_iQ_i=\Lambda \\
	\end{align}
	Setting $Q=\prod_i Q_i$
	\begin{align}
		A=Q\Lambda Q^*
	\end{align}
	Where $\Lambda$ is the diagonal matrix of these eigenvalues.

	So it is diagonalizable.
\end{exercise}

% Exercise 3.4
\begin{exercise}{3.4}
	To prove this take any matrix A. Note that A has at least one eigenpair $\lambda_1, v_1$ because the characteristic polynomial has at least one root. construct $Q_1=[v_1,q_2,\dots, q_n]$ where we complete $v_1$ to an orthonormal basis $q_2,\dots,q_n$
	\begin{align}
		Q_1^*AQ_1=Q_1 % 1 x 4 BMatrix 
		\begin{bmatrix}
			Av_1 & Aq_2 & \dots & Aq_n
		\end{bmatrix}                                                                   \\
		=Q_1 % 1 x 4 BMatrix 
		\begin{bmatrix}
			\lambda_1 v_1 & Aq_2 & \dots & Aq_n
		\end{bmatrix}                                                          \\
		=
		\begin{bmatrix}
			\langle \lambda_1v_1,v_1\rangle & \langle Aq_2,v_1\rangle & \dots  & \langle Aq_n,v_1\rangle \\
			\langle\lambda_1v_1,q_2\rangle  & \langle Aq_2,q_2\rangle & \dots  & \vdots                  \\
			\vdots                          & \vdots                  & \ddots & \vdots                  \\
			\langle\lambda_1v_1,q_n\rangle  & \dots                   & \dots  & \langle Aq_n,q_n\rangle
		\end{bmatrix} \\
		\begin{bmatrix}
			\langle \lambda_1v_1,v_1\rangle & \langle Aq_2,v_1\rangle & \dots  & \langle Aq_n,v_1\rangle \\
			0                               & \langle Aq_2,q_2\rangle & \dots  & \vdots                  \\
			\vdots                          & \vdots                  & \ddots & \vdots                  \\
			0                               & \dots                   & \dots  & \langle Aq_n,q_n\rangle
		\end{bmatrix} \\
	\end{align}
	Setting $b=[\langle Aq_2,v_1\rangle,\dots,\langle Aq_n,v_1\rangle]$:
	\begin{align}
		=% 2 x 2 BMatrix 
		\begin{bmatrix}
			\lambda_1 & b   \\
			0         & A_1
		\end{bmatrix}
	\end{align}
	We can then note that $A_1$ also has at least one eigenpair by the same argument as before. Call this $\lambda_2,v_2$. Complete $v_2$ to an orthonormal basis of $\R^{(n-1)\times (n-1)}$.

	\begin{align}
		U_2=% 1 x 4 BMatrix 
		\begin{bmatrix}
			v_2 & z_2 & \dots & z_{n-1}
		\end{bmatrix}
	\end{align}
	Set $Q_2=% 2 x 2 BMatrix 
		\begin{bmatrix}
			I & 0   \\
			0 & U_2
		\end{bmatrix}$

	From there note that:
	\begin{align}
		Q_2^*(Q_1^*AQ_1)Q_2=Q_2^*% 2 x 2 BMatrix 
		\begin{bmatrix}
			\lambda & b   \\
			0       & A_1
		\end{bmatrix}Q_2   \\
		% 2 x 2 BMatrix 
		\begin{bmatrix}
			I & 0     \\
			0 & U_2^*
		\end{bmatrix}% 2 x 2 BMatrix 
		\begin{bmatrix}
			\lambda_1 & b   \\
			0         & A_1
		\end{bmatrix}
		% 2 x 2 BMatrix 
		\begin{bmatrix}
			I & 0   \\
			0 & U_2
		\end{bmatrix}     \\
		=\begin{bmatrix}
			 I & 0     \\
			 0 & U_2^*
		 \end{bmatrix}% 2 x 2 BMatrix 
		% 2 x 2 BMatrix 
		\begin{bmatrix}
			\lambda_1 & U_2b   \\
			0         & A_1U_1
		\end{bmatrix} \\
		=% 2 x 1  & x_{12} \\
		% 2 x 2 BMatrix 
		\begin{bmatrix}
			\lambda_1 & U_2b        \\
			0         & U_2^*A_1U_2
		\end{bmatrix}
	\end{align}
	and then by the same argument as before we have that:
	\begin{align}
		% 3 x 3 BMatrix 
		\begin{bmatrix}
			\lambda_1 & *         & *   \\
			0         & \lambda_2 & *   \\
			0         & 0         & A_2
		\end{bmatrix}
	\end{align}
	Where $*$ are just arbitrary numbers that we don't care about. If we continue this process to the end we will be left with an upper triangular matrix we call T.

	From here

	\begin{align}
		(\prod_i Q_i)^*A\prod_iQ_i=T \\
	\end{align}
	Setting $Q=\prod_i Q_i$
	\begin{align}
		A=QT Q^*
	\end{align}
	From here note that since eigenvalues are invariant under unitary transformation ($U^*AU(U^*v)=U^*Av=U^*\lambda v=\lambda U^*v$, so any eigenvalue $\lambda$ of A with eigenvector $v$ is also a eigenvalue of this new matrix with eigenvector $U^*v$). The eigenvalues of A are the eigenvalues of $T$ which we can just read off of the diagonal.
\end{exercise}

% Exercise 3.5
\begin{exercise}{3.5}

	a) To show this first note that $\sup |W(A)|=\sup_{x\neq 0}|\frac{\langle Ax,x\rangle}{\norm{x}^2}|$ from here clearly note that:
	\begin{align}
		\sup_{x\neq 0}|\frac{\langle Ax,x\rangle}{\norm{x}^2}| \\
		\geq |\frac{\langle Av,v\rangle}{\norm{v}^2}|
	\end{align}
	for some arbitrary vector v. Choose v to be the eigenvector corresponding to the largest eigenvalue in magnitude.

	\begin{align}
		=| \frac{\langle Av,v\rangle}{\norm{v}^2}|
		=| \frac{\langle \lambda_{\text{max}} v,v\rangle}{\norm{v}^2}|
		=| \lambda_{\text{max}}|=\rho(A)
	\end{align}
	So the first inequality is true furthremore note that by cuachy shwartz:
	\begin{align}
		\sup_{x\neq 0}|\frac{\langle Ax,x\rangle}{\norm{x}^2}| \\
		\leq 		\sup_{x\neq 0}|\frac{||Ax||||x||}{\norm{x}^2}|  \\
		\leq 		\sup_{x\neq 0}|\frac{||Ax||}{\norm{x}}|         \\
		=||A||_2
	\end{align}
	So the second inequality holds as well.

	b) so note that W is a set function so we will view how this operates on arbitrary object in the set. Let $a\in W(e^{i \theta}A)$ that means $a=\frac{\langle v, e^{i\theta}Av\rangle}{\norm{v}^2}$ for some v. Which mean:
	\begin{align}
		a=\frac{\langle v, e^{i\theta}Av\rangle}{\norm{v}^2} \\
		=e^{i\theta}\frac{\langle v, Av\rangle}{\norm{v}^2}
	\end{align}
	Which is $e^{i\theta}b$ where $b\in W(A)$ so $a\in e^{i\theta}W(A)$. furthermore in the other direction if $a\in e^{i\theta}W(A)$ then there is a v such that:

	\begin{align}
		=e^{i\theta}\frac{\langle v, Av\rangle}{\norm{v}^2} \\
		=\frac{\langle v, e^{i\theta}Av\rangle}{\norm{v}^2}
	\end{align} which menas that $a\in W(e^{i\theta}A)$. thus $W(e^{i\theta}A)=e^{i\theta}W(A)$

	c) To do this we once again need to consider individual elements. let $a\in W(A^*)$ then there is a v such that:
	\begin{align}
		a=\frac{\langle v, A^*v\rangle}{\norm{v}^2}         \\
		=\frac{\langle Av, v\rangle}{\norm{v}^2}            \\
		=\frac{\overline{\langle v, Av\rangle}}{\norm{v}^2} \\
	\end{align}
	So thus $a\in \overline{W(A)}=W(A)^*$ (sorry I am used to the bar meaning conjugate). Conversely let $a\in W(A)^*$ then there is a v such that:
	\begin{align}
		a=\overline{\frac{\langle v, Av\rangle}{\norm{v}^2}} \\
		=\frac{\overline{\langle v, Av\rangle}}{\norm{v}^2}  \\
		=\frac{\langle Av, v}{\norm{v}^2}                    \\
		=\frac{\langle v, A^*v}{\norm{v}^2}                  \\
	\end{align}
	thus $a\in W(A^*)$ and as a result $W(A^*)=W(A)^*$

	d) lets take $a\in W(H(A))$ then there is a v with:
	\begin{align}
		a=\frac{\langle v, \frac{1}{2}(A+A^*)v\rangle}{\norm{v}^2}                                             \\
		=\frac{1}{2}(\frac{\langle v,Av\rangle}{\norm{v}^2}+\frac{\langle v,A^*v\rangle}{\norm{v}^2})          \\
		=\frac{1}{2}(\frac{\langle v,Av\rangle}{\norm{v}^2}+\overline{\frac{\langle v,Av\rangle}{\norm{v}^2}}) \\
		=\text{re}(\frac{\langle v,Av\rangle}{\norm{v}^2})
	\end{align}
	So this new set is just the real parts of $W(A)$

	So I think what I would do is since this new matrix is hermitian. its interval should be the distance between the smallest and largest eigenvalues. So as a result I would just use a numerical solver to find both of those eigenvalues and that would give me the range.

\end{exercise}

% Exercise 3.6
\begin{exercise}{3.6}

	% TODO: This technically traces out a much larger shape
	% The key to this one is part b and d of the last question. The idea is that we can only compute the range of the real part for $W(A)$ however if we rotate the range down by thetaso that the ray at angle theta now rests on the real axis, $e^{-i\theta}W(A)=W(e^{-i\theta}A)$ (part b) we can now compute the real range of that using part d of last section. From this we just then multiply that range by $e^{i\theta}$ and it will give us a vector on the boundary. We can repeat that from $\theta\in [0,2\pi)$  and that will give us some shape that should resemble the numerical range in the complex plane.

	The idea to this one is that we will compute the hermitian part of A and use that to get the real part of the a point on the numerical range. The eigenvector of this part will corresond to an eigenvector that maps to that point in the numerical range. So I can then plot the raleigh quotient of those eigenvectors. I then rotate the matrix and repeat.


	When I trace out the following matrix: I get
	\begin{align}
		% 2 x 2 BMatrix 
		\begin{bmatrix}
			1+i & 0 & 0 \\
			0   & i & 0 \\
			0   & 0 & 1
		\end{bmatrix}
	\end{align}
	\includegraphics[width=\textwidth/2]{code/jodran_block2.png}
	and everything is included in it.




	Here are a bunch of different jordan blocks of increasing dimension:

	\includegraphics[width=\textwidth/2]{code/jordans.png}

	Thus the numerical range of the jordan normal form seems to be a circle centered at the origin with radius that increases by the size of the dimension. It seems that the radius approaches one but never quite gets there.

	So for any arbitrary matrix A. I would say that the numerical range of the jordan normal form will look like circles of increasing width centered at whatever eigenvalue is degenerate. The width of the cricles is determined by the degeneracy of the eigenvalue. Furthermore taking the convex hull of all of these circles. If the point is not degenerate than the radius is just zero.
\end{exercise}
\end{document}
